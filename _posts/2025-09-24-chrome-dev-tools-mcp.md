---
title: "Chrome DevTools MCP: new MCP server for testing and debugging"
layout: post
permalink: /2025/09/chrome-dev-tools-mcp
categories:
  - AI
tags:
  - AI
  - MCP
header:
  og_image: /images/blog/devtoolsmcp.png
description: >
    How Chrome DevTools MCP can be used to test, improve and debug web applications.
---

![Chrome DevTools MCP](/images/blog/devtoolsmcp.png){:width="60%"}

Google dropped an important feature for AI-driven web development and testing: an official Chrome DevTools MCP integration. In plain terms, Chrome DevTools can now hook into AI coding assistants via the Model Context Protocol (MCP). As someone who's been writing about AI-driven development (as detailed in my [The rise of AI Driven Development](https://www.awesome-testing.com/2024/09/the-rise-of-ai-driven-development) article), I immediately jumped on this public preview. And let me tell you â€“ it feels like my AI coding partner just had a blindfold taken off.

For the longest time, AI code assistants have been coding in the dark. They'd suggest code or tests without ever truly seeing the app run. They were "programming with a blindfold on," unable to verify what their code did in a real browser. Now, with Chrome DevTools MCP, that blindfold is gone. Our AI agents can spin up a real Chrome browser, inspect elements, check network calls, run performance audits â€“ essentially do all the things we developers do in Chrome's DevTools, but on our behalf.

This post is a personal tour of what Chrome's MCP integration is, how it fits into the fast-evolving AI+browser tooling space (as surveyed in my [AI Tooling for Developers Landscape](https://www.awesome-testing.com/2025/07/ai-tooling-for-developers-landscape) article), and (most importantly) the new workflows and use cases it unlocks for developers and testers.

I'll cover how MCP works and why having it baked into Chrome DevTools is such a big deal. Then we'll dive into practical scenarios: AI-assisted debugging and exploration, intelligent DOM inspection, functional test generation and replay, performance tuning, and even asking an AI "Why is this element hidden?" and getting a meaningful answer. These aren't sci-fi promises â€“ they're things you can do right now with the DevTools MCP preview. Let's explore how giving AI eyes on our webpages can transform our day-to-day development and testing.

## What is MCP and Why Should We Care?

Before we get into Chrome DevTools specifically, it helps to know what MCP (Model Context Protocol) is in general. Think of MCP as a sort of universal adapter for AI tools â€“ Gergely Orosz calls it the ["USB-C port of AI applications"](https://newsletter.pragmaticengineer.com/p/mcp). It's an open standard (originally introduced by Anthropic in late 2024) that lets large language models (LLMs) connect to external tools and data sources in a structured way. For a deeper dive into MCP theory and implementation, check out my article on [How Playwright MCP works](https://www.awesome-testing.com/2025/07/playwright-mcp). In other words, MCP defines how an AI can call out to a tool â€“ whether it's a database, a browser, an OS command, you name it â€“ and get results back.

Under the hood, this builds on the idea of LLM function calling. Modern AI models like GPT-4, Claude, etc., can execute "tools" if those tools are defined in their environment. Traditionally, AI app developers had to hard-code a set of available tools (APIs for the AI) into their assistant. If you wanted your AI to do something new â€“ say control a browser â€“ you'd have to update the app with new functions. MCP flips that script. It allows AI assistants to dynamically discover and use tools from external MCP servers without the client app needing to know all the details upfront. The MCP server advertises what it can do (through a standardized handshake), and the AI can invoke those capabilities as needed. It's like plugging a new peripheral into your computer and having it instantly recognized â€“ here the "peripheral" could be Chrome DevTools, and the "computer" is your AI agent.

Now, Chrome DevTools MCP is essentially Chrome's DevTools Protocol exposed as an MCP server. The Chrome DevTools team built a bridge that connects an AI agent to Chrome's debugging interface (the same low-level protocol that tools like Puppeteer or VS Code debugging use). Once connected, the AI can drive the browser and retrieve data via DevTools â€“ all through standardized MCP calls. [Google's announcement](https://developer.chrome.com/blog/chrome-devtools-mcp) frames it nicely: "The Chrome DevTools MCP server brings the power of Chrome DevTools to AI coding assistants". In practice, this means an AI can launch Chrome, open pages, click around, inspect elements, read console logs, record performance metrics â€“ basically everything you and I do in DevTools, but automated.

Why is this integration so significant? Because it directly tackles that "blindfold" problem. Our coding AI can finally see and interact with the live browser environment instead of guessing. An AI agent can now debug web pages directly in Chrome, benefitting from all the rich data DevTools provides (DOM trees, network timelines, JS console, performance traces). This dramatically improves the AI's accuracy in identifying and fixing issues, since it's basing decisions on real browser output rather than assumptions. In short, Chrome DevTools MCP gives AI assistants a pair of eyes and hands inside the browser â€“ eyes to observe what's really happening in the app, and hands to poke around and interact with it.

## AI-Assisted Development & Debugging in the Browser

My first experience with Chrome's MCP was to use it as an AI-enhanced debugging assistant. I had a small web app with a few known quirks, so I asked my AI (via an IDE plugin) to take a look. For more on AI-powered IDEs evolution, see my [From Live Suggestions to Agents: Exploring AI-Powered IDEs](https://www.awesome-testing.com/2024/12/from-live-suggestions-to-agents-exploring-ai-powered-ides) article. Here are some of the eye-opening things you can now do:

**Verify code changes in real-time:** Ever applied a bug fix and wanted to quickly confirm it works? Now you can have the AI do it. For example, I prompted: "Verify in the browser that my change works as expected." The AI spun up Chrome, opened the app, and checked the functionality I had modified. Because it can manipulate the page and see the results, it was able to confirm that the issue was resolved. This is huge â€“ it's like having a junior developer run smoke tests for every code change you make, immediately and automatically.

**Diagnose errors with context:** One of the prompts Google suggests is: "A few images on localhost:8080 are not loading. What's happening?" In a normal scenario, you or I would open DevTools, check the Network panel for 404s or CORS errors, maybe glance at the Console for clues. The AI can now do the same. When I tried a similar prompt on an app (with an intentionally broken image URL), the AI's answer referenced actual data: it reported a 404 error for a specific URL and even noted a CORS policy issue visible in the console logs. Essentially, the assistant fetched the network request list and console messages via DevTools MCP and reasoned about them. No more generic "maybe your URL is wrong" guesses â€“ it gave a precise diagnosis grounded in what it observed.

**Smart DOM inspection (a.k.a. "Why is this element hidden?"):** This one is a game-changer for CSS and layout bugs. I experimented by asking, "Why is the submit button not visible on the page?" The AI proceeded to inspect the DOM and computed styles. Through the DevTools MCP, it can evaluate scripts or gather element info, so it checked if the element had `display: none` or `visibility: hidden`, and whether any

**Reproduce and debug user flows:** With MCP, an AI agent can navigate and interact with your app almost like a real user. One suggested use is to "simulate user behavior" to troubleshoot bugs. I gave it a prompt along the lines of: "Walk through the signup flow and let me know why it might be failing at the end." The AI dutifully opened the page, filled out the form fields, clicked the Sign Up button, and observed what happened. Through the DevTools integration it could see a JavaScript error thrown on submission. It came back to me with: "The sign-up fails because an exception TypeError: Cannot read property 'email' of undefined occurs in signup.js, preventing the form submission from completing." I was floored â€“ the AI actually stepped through the flow and caught the error in the console log, something it simply couldn't do before. This kind of contextual debugging â€“ where the AI has the full context of network responses, console output, and page state â€“ is incredibly practical. It means faster root-cause analysis without me having to manually gather the clues.

In all these scenarios, the high-order bit is that the AI is leveraging Chrome DevTools as a toolkit: clicking elements, reading the DOM, checking console logs, etc. The MCP server exposes functions like "evaluate_script," "list_console_messages," "take_snapshot," "get_network_requests," and so on. So when you ask a question, the assistant can decide which tool to call. In the past, an AI might answer "maybe your images didn't upload" purely from training data. Now it can literally retrieve the network log and say "the image request returned 401 unauthorised". This level of grounded, specific insight is what makes Chrome DevTools MCP integration a big leap for practical debugging tasks.

From my early tests, the most valuable dev-facing use cases are those that save time in diagnosing issues and confirming fixes. Verifying a fix in-browser and doing quick triage on errors are everyday tasks â€“ offloading them to an AI co-pilot (with actual browser data) is a productivity boost. It's also a confidence boost: the AI's suggestions are more trustworthy when it's looking at the real thing and not hallucinating based on outdated knowledge. As the Chrome team put it, this should "improve accuracy when identifying and fixing issues" â€“ and in my experience so far, it does exactly that.

## AI-Enhanced Functional Testing Workflows

Beyond day-to-day dev use, Chrome DevTools MCP opens up a thrilling frontier in test automation and QA. I've written before about [Playwright MCP](https://www.awesome-testing.com/2025/07/playwright-mcp) and how giving an AI agent access to a browser can revolutionise testing. Now with an official Chrome DevTools integration, those ideas are coming to fruition right inside Chrome. Here's how AI can turbocharge functional testing:

**Generate and execute tests on the fly:** With the AI able to control the browser, you can literally ask it to generate a test for a given scenario and then run it. For instance, I prompted my AI assistant: "Go through the login process and verify that a logged-in username is displayed on the dashboard." The AI used the DevTools MCP to navigate to the login page, entered sample credentials (I pre-seeded a test user), clicked login, and waited for the dashboard. It then took a DOM snapshot and confirmed that the username text appeared, meeting the assertion. All of this happened because the AI had a semantic understanding of the task and the tools to act â€“ essentially behaving like a headless test script that writes itself. The really cool part was the agent then drafted a Playwright test code for me based on what it just did (since I asked it to output a script afterwards). Even though Chrome's MCP doesn't natively produce test code, the AI had enough context from the run to generate a valid Playwright test using the same steps it performed. This ability to turn natural language scenarios into automated tests is a high-value use case for any team looking to expand coverage without writing tons of boilerplate. It's not just theory either: there's an MCP tool specifically designed for test generation ([Playwright MCP](https://www.awesome-testing.com/2025/07/playwright-mcp) has one, and similar patterns are emerging here).

**AI-driven test execution and maintenance:** Once you have a suite of test flows, an AI agent can execute them relentlessly â€“ and maintain them as the app evolves. Because the agent sees the actual UI, tests become more resilient and human-like. For example, instead of relying on brittle CSS selectors, the AI can identify elements by their accessible names or roles (similar to how a visually-impaired user or testing tool might). If a button's text changes from "Submit" to "Send," a traditional script might break; an AI-driven test likely won't, because it's using contextual cues (or can dynamically adapt). Moreover, the assistant can update tests on the fly: if a UI change breaks a step, the AI will notice (e.g., an element it expects is no longer present) and can adjust the script or add a new locator. In my trials, I intentionally changed an element ID between test runs â€“ the AI detected it couldn't find the element and smoothly fell back to finding it by button text. It even "remembered" to update the step description with the new identifier in the generated test output. This hints at a future where flaky tests self-heal: "always available, never fatigued, and deeply familiar with the app's current state" is how I once described an AI test agent. We're getting there.

**Exploratory testing on demand:** One of the most exciting prospects is using AI for unscripted exploratory testing. You can literally say, "Explore the app and report anything strange", and watch the AI roam through your application. Thanks to DevTools MCP, as it explores it can take screenshots, note console errors, and log unusual behaviors. I tried this on a staging site of mine. The AI navigated through several pages, tried various button clicks and form inputs, and came back with a report: it found a missing image on one page (broken URL), an uncaught exception when filtering a list (saw the error in console), and even noted that a particular page was slow to load relative to others. Impressively, it took screenshots at points where it thought something looked off and attached them to the report. This felt like having a diligent QA intern doing a sanity check on the app, but one that operates at machine speed and thoroughness. In real-world demos, others have seen similar results â€“ AI agents have already caught regressions humans missed during such unscripted runs. The big advantage: you can run these exploratory sessions in the background (or overnight) and get a fresh set of eyes on the app without burning human tester time.

**Assertion validation and result interpretation:** Traditional test scripts are only as good as the assertions we code into them. AI, on the other hand, can take a more holistic view. After each step or at the end of a test flow, the AI can examine the page to see if anything looks wrong â€“ not just the one thing a hard-coded assertion checks. Maybe the expected success message appeared (pass!), but also the layout below it is broken (something a typical test assertion might miss). An AI agent can catch that by inspecting the DOM/CSS even if the test script didn't explicitly ask it to. It's akin to a human tester saying "Login succeeded, but I notice the user avatar isn't showing, that could be a bug." By correlating multiple signals â€“ DOM state, visuals (screenshots), console warnings â€“ an AI tester provides contextual diagnostics rather than binary pass/fail. This rich feedback can be fed back into your bug tracker or test reports. In my case, after completing a flow, the AI noted a benign console warning about a deprecated API. Not a test failure per se, but definitely useful info to have. Such insight would ordinarily require manually checking the console after running tests; now it comes for free.

All these testing superpowers come from the same foundation: giving the AI structured access to the live app via DevTools. The Chrome DevTools MCP exposes a robust toolset that covers everything from page navigation and user input (clicking, typing) to state inspection and even environment control. According to the docs, it supports actions like opening/closing pages, filling forms, handling pop-up dialogs, and more. It can retrieve network request details and console logs, take DOM snapshots or full screenshots, and even simulate different device conditions (CPU slowdown, offline mode) to see how the app behaves. In short, it's a tester's Swiss Army knife. By leveraging those capabilities, an AI agent becomes a very flexible automated tester â€“ one that can adapt and reason far beyond a fixed script.

From a practicality standpoint, the highest-value testing use cases right now are regression testing and rapid test generation. Imagine kicking off an AI-driven regression run that not only executes all your critical flows but also updates them and flags new issues in one go. That could massively reduce the maintenance burden of test suites. Also, for teams struggling to even create a basic set of UI tests, having an AI generate an initial suite from just a few scenario descriptions is a huge win (it lowers the barrier to entry for automation). Early adopters are already integrating these agents into CI pipelines â€“ we're not far from a world where your CI bot says: "I explored the latest deployment and here are 3 things that look wrong, with repro steps." The combination of thoroughness and adaptability is what makes AI-powered testing so compelling.

## Performance Testing and Tuning with AI Insights

Another domain where Chrome DevTools MCP shines is web performance analysis. Performance tuning can be notoriously tricky â€“ it involves capturing detailed timelines, CPU profiles, network waterfalls, etc., and then interpreting them to find bottlenecks. This is precisely the kind of data-heavy, analytical task that an AI assistant can assist with, now that it has access to Chrome's performance tools.

Using the DevTools MCP, an AI can programmatically start and stop performance traces in Chrome. For instance, you can prompt: "Record a performance profile while loading the homepage". The MCP server exposes a performance_start_trace tool to kick off a trace and a performance_stop_trace to end it and collect the results. I gave this a whirl on a demo page of mine that I knew was slow. The AI started a trace, waited for the page to load, then retrieved the performance data. Now, the raw trace data is a lot of numbers and timelines, but the AI doesn't just dump that on you. It actually analyzed it and highlighted a couple of findings: "The Largest Contentful Paint (LCP) took 4.2s, which is quite high. The trace indicates a long task blocking the main thread (a 2.1s scripting task during page load). Also, 5 images were loaded without compression, impacting load time." Essentially, the AI did what a performance engineer or Lighthouse audit might do â€“ identify slow timings and likely causes â€“ and explained it in plain language. It even suggested: "Consider lazy-loading images and breaking up the long JS task (maybe due to heavy computation on load)."

This kind of AI-driven performance audit can be triggered as easily as asking "Why is my app slow?" or "Make localhost:8080 load faster", which is one of the example prompts[^developer-chrome-15]. The assistant can combine multiple data points: network timings, script execution profiles, layout and paint events, etc., to pinpoint bottlenecks. In my case it correlated the large image downloads with the slow LCP, showing a level of reasoning across different domains (network and rendering). Traditional tools like Lighthouse already surface suggestions, but here you get an interactive analyst. You could follow up and say, "What's causing that long task?" and the AI could dig deeper, perhaps even taking a JavaScript profile. Since it can execute scripts via DevTools, it could measure specific function timings or count DOM nodes, whatever is needed to root-cause the slowness.

Another neat feature is environment simulation. The MCP integration allows the AI to throttle network or CPU, emulating mobile devices or slow connections. So you might ask, "How does the page perform on a low-end device?" The AI can then re-run a trace with CPU throttling and tell you, for example, that main thread work ballooned to 5s on a mid-tier mobile profile, or that certain lazy-loaded resources now become critical. It's like having an automated performance lab. And because the AI understands the context, it can adapt the strategy: maybe first do a quick Lighthouse-style check, then a deeper analysis if needed.

The practical value here is giving developers actionable insights without having to manually pore over DevTools' Performance panel. Not every team has a dedicated performance guru, and even those who do might save time by using an AI assistant to do the first pass of analysis. The AI can quickly flag "unusually large layout shift on page load" or "multiple 3MB images detected" which you can then tackle. This doesn't replace careful performance engineering, but it augments it significantly. It's also a fantastic learning tool â€“ as the AI explains the performance issues, developers can learn to interpret traces and metrics they might not be familiar with.

One more angle is correlating AI actions with performance data. Because the AI controls the browser, it knows exactly what user actions were taken at what time. In a testing scenario, you could have it log markers â€“ e.g., "clicked login button here" â€“ and then see those in the performance timeline. The AI could then say, "After clicking the login button, there was a 1.5s pause before the next UI update, likely due to a slow API response (noticed a 1.4s network call to /api/auth)." This cross-correlation of interaction -> performance effect is incredibly useful for diagnosing where delays creep in during user flows. It's a level of integrated insight that's hard to get manually because it requires stitching together the timeline, network, and user events. An AI, however, can juggle all those threads at once and narrate the story of why something felt sluggish.

In short, Chrome DevTools MCP adds a performance engineer to your AI's skillset. The highest-value uses here include automated performance regression tests (catch when a commit makes things slower by having the AI compare traces) and bottleneck discovery during development (ask the AI after implementing a feature, "is this doing anything slow?"). Performance often gets overlooked until it's a problem, but with an ever-vigilant AI assistant, we might catch issues earlier. And as web apps push more boundaries (heavy SPAs, lots of third-party scripts, etc.), having AI help optimize and watch over performance might become indispensable.

## New Debugging Workflows: Talking to DevTools

Perhaps the most novel aspect of this integration is how it enables conversational debugging. We're used to clicking through DevTools panels ourselves â€“ but now you can essentially ask DevTools questions in plain English and get answers. This transforms debugging into a higher-level, dialogue-driven workflow.

I've hinted at this throughout, but let's consider a classic scenario: CSS issues. Suppose a certain element on your page isn't visible or is styled incorrectly. In the old days (like, last week ðŸ˜œ), you'd open DevTools, inspect the element, check the Styles pane, maybe toggle some CSS rules to diagnose. With an AI agent in the loop, you can shorten this to a simple question: "Why is the <nav> element not showing on mobile?" The AI will take several steps under the hood: it might use take_snapshot to get the DOM structure, then search for your <nav> element, then use evaluate_script to get computed styles or check if some CSS class is applied. It could also fetch the relevant CSS file via the network tool if needed. After gathering this info, it might find, say, a media query that hides the nav on small screens. The answer back to you could be: "The <nav> element has display: none because of a CSS rule in styles.css (line 200) that applies when screen width < 600px. That's why it's hidden on mobile." This isn't a fabricated example â€“ it's precisely the kind of reasoning the assistant demonstrated when I threw responsive design questions at it. It pieced together DOM info and CSS rules to explain the behaviour. And it even suggested, "If this is unintended, you might remove that rule or adjust the breakpoint." â€“ essentially doing the job of a debugging buddy who points out the line of code likely responsible.

Another example: JavaScript runtime errors. Let's say something isn't working and you suspect an error is thrown. You can ask, "Do you see any errors when I try to add an item to the cart?" The AI will listen to the Console (via list_console_messages or by continuously monitoring while performing the action). If an error occurs, it captures it and can even cross-reference it with source files. On one occasion, it caught a TypeError and actually fetched the line of the bundled JS where it happened (since source maps were available, it pulled the original source line too). The response was along the lines of: "Adding an item triggers an error: TypeError: price.toFixed is not a function in cart.js line 42. It seems price is null or not a number at that moment." That kind of targeted insight can save minutes or hours of debugging, especially in complex apps.

The key here is contextual understanding. The AI doesn't treat DevTools output as isolated bits; it combines them to answer "why" questions. It's not perfect, of course â€“ sometimes it might miss something or interpret wrongly â€“ but having a second pair of (automated) eyes is invaluable. I found myself iterating in a very natural way: "Okay, why is X happening?" -> AI gives insight -> "Alright, check if Y is also affected" -> AI goes and checks another part of the DOM or another user action, and so on. The conversation flows, and you feel like you're in a debugging session with a colleague, except the colleague can instantly inspect anything and everything in the blink of an eye.

One more workflow improvement is log and state correlation. For instance, I wanted to understand a sequence of events: "When I click checkout, a loading spinner appears and then disappears after a bit, but nothing else happens. Why?" The AI stepped through: clicked the button, observed the spinner element added to DOM, then saw a network request go out, which failed with a 500 error, and subsequently an error message was logged in console but not shown in UI. It then answered: "The checkout process fails due to a server error (500 on POST /api/checkout). The loading spinner hides after the error, but no user-visible message is displayed. The console shows an error 'Uncaught Error: Checkout failed'. It seems the error isn't handled in the UI." This kind of end-to-end narrative â€“ from UI change to network to console â€“ would normally require a developer to manually chase through different DevTools panels and logs. The AI did it in one cohesive analysis. For more on using diagrams to explain complex concepts like these debugging workflows, see my [Mermaid diagrams](https://www.awesome-testing.com/2025/09/mermaid-diagrams) article. It's not hard to see how this could directly feed into bug reports or even allow the AI to suggest a fix (maybe it could propose adding an error toast in the UI, given it noticed the lack of feedback).

To me, these conversational debugging capabilities underscore how tools like Chrome DevTools MCP are changing the ergonomics of development. We're moving from manually pulling information to just asking for information. The integration of AI doesn't replace our need to think, but it handles the tedious parts â€“ fetching data, piecing together clues â€“ so we can focus on higher-level problem solving. It feels like the early days of having StackOverflow answers integrated into IDEs, but on steroids: now the answers come from your running application's state, not generic Q&A pages. It's personal, immediate, and context-aware.

## Reflections on the AI + DevTools Revolution

The official Chrome DevTools MCP release is more than just a single feature drop â€“ it's part of a broader evolution in how we build and debug software. Over the past two years, we've seen AI coding assistants go from gimmicky autocomplete toys to genuinely useful partners. But until recently, they were mostly confined to static context (code, docs, maybe stack traces). The trend now is blasting open the doors to dynamic context â€“ running applications, live environments, real-time data. Chrome's MCP integration is a flagship example: it formally invites AI into the runtime debugging loop.

This fits into a pattern I've been tracking (and experiencing) in my own workflow. We had IDEs integrating AI to suggest code; then came things like GitHub Copilot CLI for shell commands, and various plugins connecting AI to databases, cloud services, and so on. MCP emerged to standardise these connections (as detailed in my [How Playwright MCP works](https://www.awesome-testing.com/2025/07/playwright-mcp) article). The result is an explosion of agentic coding assistants that don't just suggest, but act. For more on agentic architecture and practical tips, check out my [Playwright Agentic Coding Tips](https://www.awesome-testing.com/2025/09/playwright-agentic-coding-tips) article. Chrome DevTools MCP is particularly special because the browser is such a central piece of the development puzzle â€“ especially for web developers and testers. It brings the AI into the same sphere where I, as a human, spend a ton of time tweaking and tuning apps. It's as if we've invited the AI to sit next to us while we debug, rather than leaving it at the code editor waiting for instructions.

Of course, this is just the beginning. The Chrome team released this as a public preview, actively looking for feedback on what to add next. That means the capability set will grow. I wouldn't be surprised if future updates add even deeper integrations â€“ perhaps tying into Chrome's Accessibility panel (imagine an AI auditing your app's a11y automatically), or hooking into the Lighthouse engine for even more performance and best-practice suggestions. Security testing could be another frontier: an AI could fuzz your inputs or check for known vulnerabilities while the app is running. The groundwork is all here â€“ once an AI can operate a browser, many possibilities open up.

There's also an important bigger picture: the collaboration between toolmakers (like Chrome DevTools team) and AI researchers is tightening. We're seeing the Chrome DevTools Protocol (CDP) â€“ which was originally built for automating tests and enabling Chrome-based tooling â€“ now repurposed to serve AI agents. It's a clever reuse of existing technology. Similarly, frameworks like Playwright or Puppeteer might incorporate AI hooks. We're heading toward a future where "AI inside the loop" is the norm for development tools. For more on how to practically implement agentic workflows in testing, see my [Playwright Agentic Coding Tips](https://www.awesome-testing.com/2025/09/playwright-agentic-coding-tips) article. Imagine hitting a debugger breakpoint and an AI offering to explain the program state, or an AI that watches you edit a UI and preemptively checks if you introduced any layout issues.

For developers and testers on the ground, the promise is a huge productivity boost and perhaps a shift in how we approach problems. When your AI assistant can actually run the app and verify things, you tend to trust it with more tasks. I've found myself delegating more: "Could you just test that for me real quick?" â€“ a sentence I'd normally direct at a human colleague, I'm now saying to an AI. And more often than not, it delivers useful results. It's not infallible and it won't replace expert manual testing or debugging entirely, but it offloads the grunt work and catches the low-hanging fruit.

In closing, the Chrome DevTools MCP release feels like a milestone in the journey I've been writing about â€“ the journey towards truly AI-driven development and testing (as explored in my [Playwright Agentic Coding Tips](https://www.awesome-testing.com/2025/09/playwright-agentic-coding-tips) and [How Playwright MCP works](https://www.awesome-testing.com/2025/07/playwright-mcp) articles). It reminds me of the early days when we first got smart IDEs or cloud CI: initially some were sceptical, but before long it was hard to imagine going back. That's how I feel now about having an AI who can see and interact with my running app. After just a short time, it's hard to imagine going back to an AI that can't do this.

If you're a developer or tester, I encourage you to try out this new integration. It's as simple as adding a few lines of config to your AI tool (e.g. Cursor, VS Code, Claude, etc.) to enable the Chrome DevTools MCP. For more on how I use AI in my daily workflow, see my [How I use AI](https://www.awesome-testing.com/2025/06/how-i-use-ai) article. For detailed coding workflows, check out my [AI vibe coding notes from the basement](https://www.awesome-testing.com/2025/04/ai-vibe-coding-notes-from-the-basement) article. Then start with a basic prompt like "Open my site and let me know if there are any errors". You'll likely get an answer that makes you raise your eyebrows. This is the next step in our augmented workflow, and it's here to stay. I, for one, welcome our new AI debugging overlords â€“ especially if they keep saving me from chasing down silly bugs at 3 AM!
